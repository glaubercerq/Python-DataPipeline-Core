# ğŸš€ Pipeline ETL - Data Engineering Project

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Pandas](https://img.shields.io/badge/Pandas-2.1.4-green)
![SQLAlchemy](https://img.shields.io/badge/SQLAlchemy-2.0.23-orange)
![Status](https://img.shields.io/badge/Status-Production-success)

## ğŸ“‹ Sobre o Projeto

Este projeto implementa um **pipeline ETL (Extract, Transform, Load)** completo em Python, simulando um fluxo de dados real de uma empresa de e-commerce. O sistema extrai dados de vendas de mÃºltiplas fontes, realiza transformaÃ§Ãµes complexas com conversÃ£o de moedas e carrega os resultados em um Data Warehouse SQLite.

### ğŸ¯ Problema de NegÃ³cio

**Desafio:** Uma empresa de e-commerce vende produtos em Real (BRL) mas precisa reportar resultados em DÃ³lar (USD) para a matriz internacional.

**SoluÃ§Ã£o ETL:**
- **Extract (E):** Coleta dados de vendas locais (CSV) e taxas de cÃ¢mbio em tempo real (API)
- **Transform (T):** Converte preÃ§os BRLâ†’USD, calcula mÃ©tricas agregadas e garante qualidade dos dados
- **Load (L):** Armazena dados transformados em banco de dados para anÃ¡lises de BI

---

## ğŸ—ï¸ Arquitetura do Projeto

```
etl_project/
â”œâ”€â”€ data/                      # Dados do projeto
â”‚   â”œâ”€â”€ raw/                  # Dados originais (CSV)
â”‚   â”œâ”€â”€ processed/            # Dados transformados (opcional)
â”‚   â””â”€â”€ database/             # Banco SQLite (modo local)
â”œâ”€â”€ etl_scripts/              # Scripts ETL
â”‚   â”œâ”€â”€ __init__.py          # Inicializador do pacote
â”‚   â”œâ”€â”€ config.py            # ConfiguraÃ§Ãµes centralizadas
â”‚   â”œâ”€â”€ extract.py           # MÃ³dulo de ExtraÃ§Ã£o
â”‚   â”œâ”€â”€ transform.py         # MÃ³dulo de TransformaÃ§Ã£o
â”‚   â”œâ”€â”€ load.py              # MÃ³dulo de Carregamento
â”‚   â””â”€â”€ main_pipeline.py     # Orquestrador Principal
â”œâ”€â”€ init-scripts/             # Scripts de inicializaÃ§Ã£o DB
â”œâ”€â”€ docker-compose.yml        # ConfiguraÃ§Ã£o Docker
â”œâ”€â”€ Dockerfile               # Container da aplicaÃ§Ã£o
â”œâ”€â”€ .env                     # VariÃ¡veis de ambiente
â”œâ”€â”€ requirements.txt         # DependÃªncias Python
â””â”€â”€ README.md                # DocumentaÃ§Ã£o
```

### **Bancos de Dados Suportados**

- **SQLite** (padrÃ£o): Arquivo local, ideal para desenvolvimento
- **PostgreSQL** (Docker): Banco profissional, ideal para produÃ§Ã£o

### **Modos de ExecuÃ§Ã£o**

- **Local**: Python + SQLite (simples, rÃ¡pido)
- **Containerizado**: Docker + PostgreSQL (profissional, escalÃ¡vel)

---

## ğŸ› ï¸ Tecnologias Utilizadas

| Ferramenta | Uso | Conceito Chave |
|------------|-----|----------------|
| **Pandas** | ManipulaÃ§Ã£o e transformaÃ§Ã£o de dados | DataFrames, Limpeza, AgregaÃ§Ã£o |
| **SQLAlchemy** | ConexÃ£o e ORM para banco de dados | Engine, to_sql(), Multi-DB support |
| **SQLite** | Banco de dados relacional local | Data Warehouse simplificado |
| **PostgreSQL** | Banco de dados profissional | ProduÃ§Ã£o, escalabilidade |
| **Docker** | ContainerizaÃ§Ã£o da aplicaÃ§Ã£o | Isolamento, portabilidade |
| **Docker Compose** | OrquestraÃ§Ã£o de containers | Multi-service setup |
| **Requests** | IntegraÃ§Ã£o com APIs externas | HTTP GET, JSON parsing |
| **Logging** | Monitoramento e rastreabilidade | Logs estruturados |

---

## ğŸ“Š Fontes de Dados

### 1. **Dataset Local (CSV)**
- **Arquivo:** `data/raw/vendas.csv`
- **ConteÃºdo:** Dados transacionais de vendas (51 registros)
- **Colunas:** Data_Venda, Produto, Categoria, Quantidade, Preco_Local (BRL), Regiao

### 2. **APIs de CÃ¢mbio (Multi-Source Failover)** ğŸ†•
- **API PrimÃ¡ria:** Frankfurter (Banco Central Europeu)
- **API SecundÃ¡ria:** ExchangeRate-API
- **API TerciÃ¡ria:** Fixer.io
- **FunÃ§Ã£o:** Taxa de conversÃ£o BRL â†’ USD em tempo real
- **EstratÃ©gia:** Sistema de failover em cascata (99.9% disponibilidade)
- **Tipo:** APIs pÃºblicas gratuitas

### 3. **APIs de Criptomoeda (Multi-Source Failover)** ğŸ†•
- **API PrimÃ¡ria:** CoinGecko (â­ Recomendada - Dados completos)
- **API SecundÃ¡ria:** Binance (AltÃ­ssima velocidade)
- **API TerciÃ¡ria:** CoinCap (Dados em tempo real)
- **API QuaternÃ¡ria:** CoinDesk (Backup final)
- **FunÃ§Ã£o:** CotaÃ§Ã£o Bitcoin em USD, EUR, GBP, BRL
- **EstratÃ©gia:** Failover inteligente com 4 fontes confiÃ¡veis
- **Tipo:** APIs pÃºblicas gratuitas
- **AtualizaÃ§Ã£o:** Dados em tempo real (nÃ£o usa fallback estÃ¡tico)

> **ğŸ’¡ Destaque TÃ©cnico:** Implementamos **sistema profissional de failover** que tenta mÃºltiplas APIs em ordem de prioridade, garantindo que sempre obtemos dados reais e atualizados. Veja detalhes completos em [`API_INTEGRATION.md`](API_INTEGRATION.md).

---

## ğŸ”„ Fluxo do Pipeline

### **Fase 1: ExtraÃ§Ã£o (Extract)**
```python
# extract.py
- LÃª vendas.csv usando pd.read_csv()
- Chama API de cÃ¢mbio (BRLâ†’USD)
- Chama API de criptomoeda (Bitcoin)
- Retorna: DataFrame + metadados de APIs
```

**Conceitos demonstrados:**
- Leitura de mÃºltiplas fontes
- Tratamento de requisiÃ§Ãµes HTTP
- ValidaÃ§Ã£o de dados externos
- Sistema de fallback para APIs indisponÃ­veis

### **Fase 2: TransformaÃ§Ã£o (Transform)**
```python
# transform.py
1. Limpeza:
   - Remove duplicatas
   - Trata valores nulos
   - Padroniza tipos de dados

2. Enriquecimento:
   - Converte Preco_Local (BRL) â†’ Preco_USD
   - Calcula Valor_Total_USD = Quantidade Ã— Preco_USD
   - Extrai features de tempo (Ano, MÃªs, Dia da Semana)

3. AgregaÃ§Ã£o:
   - Agrupa por Data_Venda
   - Calcula mÃ©tricas: Total_Vendas_USD, Ticket_Medio, etc.
```

**Conceitos demonstrados:**
- ManipulaÃ§Ã£o avanÃ§ada de DataFrames
- CriaÃ§Ã£o de features (Feature Engineering)
- AgregaÃ§Ãµes complexas com groupby()
- Garantia de qualidade de dados

### **Fase 3: Carregamento (Load)**
```python
# load.py
- Conecta ao SQLite usando SQLAlchemy
- Cria tabelas: vendas_detalhadas, vendas_agregadas
- Usa df.to_sql() com modo replace/append
- Registra metadados de execuÃ§Ã£o
```

**Conceitos demonstrados:**
- Uso de ORM (SQLAlchemy)
- EstratÃ©gias de carga (incremental vs full)
- ValidaÃ§Ã£o pÃ³s-carga
- Auditoria e rastreabilidade

---

## ğŸš€ Como Executar

### **OpÃ§Ã£o 1: ExecuÃ§Ã£o Local (SQLite)**

#### **1. PrÃ©-requisitos**
- Python 3.8 ou superior
- pip (gerenciador de pacotes)

#### **2. InstalaÃ§Ã£o**

```powershell
# Clone ou baixe o projeto
cd Python-DataPipeline-Core

# Crie um ambiente virtual (recomendado)
python -m venv .venv

# Ative o ambiente virtual
.venv\Scripts\Activate.ps1  # Windows PowerShell
# ou
.venv\Scripts\activate.bat  # Windows CMD

# Instale as dependÃªncias
pip install -r requirements.txt
```

#### **3. Executar o Pipeline**

```powershell
# Navegue atÃ© a pasta do projeto
cd etl_scripts

# Execute o pipeline principal
python main_pipeline.py
```

### **OpÃ§Ã£o 2: ExecuÃ§Ã£o com Docker (PostgreSQL)** ğŸ³

#### **1. PrÃ©-requisitos**
- Docker e Docker Compose instalados

#### **2. Executar com Docker Compose**

```bash
# Na raiz do projeto, execute:
docker-compose up --build

# Ou para executar em background:
docker-compose up -d --build
```

#### **3. Verificar os Logs**

```bash
# Ver logs do pipeline
docker-compose logs etl_app

# Ver logs do PostgreSQL
docker-compose logs postgres
```

#### **4. Conectar ao Banco PostgreSQL**

```bash
# Conectar via psql dentro do container
docker-compose exec postgres psql -U etl_user -d sales_datawarehouse

# Ou conectar externamente (porta 5432)
psql -h localhost -p 5432 -U etl_user -d sales_datawarehouse
```

#### **5. Parar os Containers**

```bash
# Parar e remover containers
docker-compose down

# Parar e remover volumes tambÃ©m
docker-compose down -v
```

### **4. SaÃ­da Esperada**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘             ğŸš€ PIPELINE ETL - DATA ENGINEERING ğŸš€           â•‘
â•‘  Extract â†’ Transform â†’ Load                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FASE 1/3: EXTRAÃ‡ÃƒO DE DADOS
ğŸ“‚ Extraindo dados do CSV...
âœ… 51 registros extraÃ­dos
ğŸŒ Extraindo taxa de cÃ¢mbio: BRL â†’ USD
âœ… Taxa: 1 BRL = 0.20 USD

FASE 2/3: TRANSFORMAÃ‡ÃƒO DE DADOS
ğŸ§¹ Limpeza concluÃ­da: 51 registros vÃ¡lidos
ğŸ’ PreÃ§os convertidos para USD
ğŸ“Š AgregaÃ§Ã£o concluÃ­da: 17 perÃ­odos

FASE 3/3: CARREGAMENTO
ğŸ’¾ Tabela 'vendas_detalhadas' carregada: 51 registros
ğŸ’¾ Tabela 'vendas_agregadas' carregada: 17 registros
âœ… PIPELINE CONCLUÃDO COM SUCESSO!
```

---

## ğŸ“ˆ Resultados

ApÃ³s a execuÃ§Ã£o, o pipeline cria:

### **1. Banco de Dados SQLite**
- **LocalizaÃ§Ã£o:** `data/database/sales_datawarehouse.db`
- **Tabelas criadas:**
  - `vendas_detalhadas` - Todas as transaÃ§Ãµes com conversÃ£o USD
  - `vendas_agregadas` - Resumo diÃ¡rio de vendas
  - `etl_metadata` - HistÃ³rico de execuÃ§Ãµes do pipeline

### **2. MÃ©tricas Calculadas**
- Total de Vendas em USD por dia
- Ticket MÃ©dio
- NÃºmero de TransaÃ§Ãµes
- Produtos Ãºnicos vendidos
- Quantidade total de itens

### **3. Logs de ExecuÃ§Ã£o**
- **Arquivo:** `etl_pipeline.log`
- ContÃ©m histÃ³rico completo de todas as execuÃ§Ãµes

---

## ğŸ§ª Testes Individuais

Cada mÃ³dulo pode ser testado separadamente:

```powershell
# Testar extraÃ§Ã£o
python extract.py

# Testar transformaÃ§Ã£o
python transform.py

# Testar carregamento
python load.py
```

---

## ğŸ“ Exemplo de Consulta SQL

ApÃ³s executar o pipeline, vocÃª pode consultar os dados:

```sql
-- Top 5 dias com maior faturamento
SELECT 
    Data_Venda,
    Total_Vendas_USD,
    Numero_Transacoes,
    Ticket_Medio_USD
FROM vendas_agregadas
ORDER BY Total_Vendas_USD DESC
LIMIT 5;

-- Vendas por categoria
SELECT 
    Categoria,
    COUNT(*) as Total_Vendas,
    SUM(Valor_Total_USD) as Faturamento_USD
FROM vendas_detalhadas
GROUP BY Categoria
ORDER BY Faturamento_USD DESC;
```

---

## ğŸ“ Conceitos de Engenharia de Dados Demonstrados

### âœ… **Boas PrÃ¡ticas Implementadas**
1. **Modularidade:** SeparaÃ§Ã£o clara de responsabilidades (E-T-L)
2. **Tratamento de Erros:** Try-except em todas as operaÃ§Ãµes crÃ­ticas
3. **Logging:** Rastreabilidade completa do processo
4. **ValidaÃ§Ã£o:** VerificaÃ§Ã£o de dados em cada etapa
5. **Fallback:** Sistema resiliente a falhas de API
6. **DocumentaÃ§Ã£o:** CÃ³digo comentado e README completo

### ğŸ“š **Habilidades TÃ©cnicas**
- ManipulaÃ§Ã£o de DataFrames com Pandas
- IntegraÃ§Ã£o com APIs REST
- Modelagem de dados para Data Warehouse
- Uso de ORM (SQLAlchemy)
- EstratÃ©gias de carga (Full Load vs Incremental)
- Feature Engineering
- AgregaÃ§Ãµes e cÃ¡lculos complexos

---

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### **Modo de Carga**
No arquivo `main_pipeline.py`, vocÃª pode alterar:

```python
LOAD_MODE = 'replace'  # Substitui dados existentes
# ou
LOAD_MODE = 'append'   # Adiciona novos dados (carga incremental)
```

### **PersonalizaÃ§Ã£o de Taxas**
Edite em `extract.py`:

```python
exchange_rate = extract_exchange_rate_api(
    base_currency='BRL',
    target_currency='USD'
)
```

---

## ğŸ“Š VisualizaÃ§Ã£o dos Dados

VocÃª pode conectar ferramentas de BI ao banco SQLite:
- **DBeaver** (visualizador SQL)
- **Power BI** (dashboards)
- **Tableau** (anÃ¡lises visuais)
- **Python Jupyter Notebook** (anÃ¡lises exploratÃ³rias)

---

## ğŸ¤ ContribuiÃ§Ãµes

Este Ã© um projeto educacional desenvolvido para demonstrar competÃªncias em:
- Engenharia de Dados
- Pipeline ETL
- Python para Data Engineering
- IntegraÃ§Ã£o de Sistemas

---

## ğŸ“„ LicenÃ§a

Projeto desenvolvido para fins educacionais e de portfÃ³lio.

---

## ğŸ‘¤ Autor

Desenvolvido como projeto de demonstraÃ§Ã£o de habilidades em **Data Engineering** e **ETL Pipelines**.

---

## ğŸ“ Suporte

Para dÃºvidas sobre o projeto:
- Consulte os comentÃ¡rios no cÃ³digo
- Analise os logs de execuÃ§Ã£o
- Verifique a documentaÃ§Ã£o das bibliotecas utilizadas

---

**ğŸ¯ Objetivo AlcanÃ§ado:** Pipeline ETL completo, funcional e pronto para apresentaÃ§Ã£o em entrevistas tÃ©cnicas!
